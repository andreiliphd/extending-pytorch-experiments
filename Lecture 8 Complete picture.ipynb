{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch custom derivative test\n",
    "\n",
    "A test about how PyTorch takes derivatives w.r.t. x for a backpropagation if we have more than two layers. Custom functions allow completely change a derivation behavior of PyTorch. Probably x has a special attribute inside PyTorch that marks it as an input. PyTorch tracks that.\n",
    "\n",
    "Please, pay attention to the following part of the code:\n",
    "\n",
    "```\n",
    "            grad_input = grad_output.mm(weight) + dumb # ADDED INTENTIONALLY\n",
    "\n",
    "```\n",
    "This is an incorrect derivation made intentionally to check how PyTorch behave in this case. We can see that backproping to the first layer takes a total derivative w.r.t x.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb = torch.randn([100, 64])\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t()) #         + torch.ones([100, 64])\n",
    "        print(output.shape)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        print('grad_ouput', grad_output)\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            global dumb\n",
    "            grad_input = grad_output.mm(weight) + dumb # ADDED INTENTIONALLY\n",
    "            print(grad_input.shape)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = Linear(3,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones([100,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_x = 1\n",
    "class FC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc1 = Linear(3,64,bias=True)\n",
    "        self.fc2 = Linear(64,128,bias=True)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        x = self.fc1(x)\n",
    "        print('A first layer output', x)\n",
    "        global first_layer_x\n",
    "        first_layer_x = x\n",
    "        print('First layer x', first_layer_x)\n",
    "        print('x', x)\n",
    "        x = self.fc2(x)\n",
    "        print('A second layer output', x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
       "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
       "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
       "        ...,\n",
       "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
       "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
       "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.matmul(param[0].T) + param[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64])\n",
      "A first layer output tensor([[-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        ...,\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155]],\n",
      "       grad_fn=<LinearFunctionBackward>)\n",
      "First layer x tensor([[-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        ...,\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155]],\n",
      "       grad_fn=<LinearFunctionBackward>)\n",
      "x tensor([[-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        ...,\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155]],\n",
      "       grad_fn=<LinearFunctionBackward>)\n",
      "torch.Size([100, 128])\n",
      "A second layer output tensor([[ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
      "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
      "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
      "        ...,\n",
      "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
      "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
      "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524]],\n",
      "       grad_fn=<LinearFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
       "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
       "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
       "        ...,\n",
       "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
       "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524],\n",
       "        [ 0.0616, -0.0346, -0.0717,  ...,  0.0043, -0.0171,  0.1524]],\n",
       "       grad_fn=<LinearFunctionBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, torch.ones([100,128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0228, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss calculation according to: https://pytorch.org/docs/stable/_modules/torch/nn/functional.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0228, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.pow(output - torch.ones([100, 128]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivative of `torch.mean(torch.pow(output - torch.ones([100, 64]), 2))` is 2 * (output - torch.ones([100,64]))/6400. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*128/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        ...,\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output - torch.ones([100, 128]))/6400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gradient = ((output - torch.ones([100, 128]))/6400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        ...,\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
       "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_ouput tensor([[-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
      "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
      "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
      "        ...,\n",
      "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
      "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001],\n",
      "        [-0.0001, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0001]])\n",
      "torch.Size([100, 64])\n",
      "grad_ouput tensor([[ 0.0914, -1.4854,  0.1134,  ..., -0.2377,  1.0037, -0.7683],\n",
      "        [ 0.4085,  0.1006, -0.6509,  ...,  1.7163,  0.9196, -0.5435],\n",
      "        [-0.5303,  0.8741,  1.5429,  ..., -1.2192,  0.7187,  0.7971],\n",
      "        ...,\n",
      "        [-2.1106,  0.2159, -0.4652,  ...,  0.4250, -1.3479, -0.2294],\n",
      "        [ 0.9369, -0.7744,  2.0606,  ..., -0.8292, -0.5899, -0.1206],\n",
      "        [-1.0319,  0.6146, -0.3172,  ...,  0.2297, -1.4789,  1.0998]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0806, -0.0817, -0.0623,  ...,  0.0449,  0.0076, -0.0028],\n",
      "        [ 0.0179, -0.0492,  0.0152,  ..., -0.0833,  0.0754, -0.0307],\n",
      "        [ 0.0373,  0.0031, -0.0319,  ...,  0.0046, -0.0867,  0.0502],\n",
      "        ...,\n",
      "        [-0.0001,  0.0490,  0.0659,  ..., -0.0413, -0.0097, -0.0660],\n",
      "        [ 0.0660, -0.0291, -0.0094,  ..., -0.0817, -0.0620,  0.0590],\n",
      "        [ 0.0987,  0.0350,  0.0048,  ..., -0.0320, -0.0627, -0.0613]],\n",
      "       requires_grad=True)\n",
      "tensor([[-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        ...,\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155],\n",
      "        [-0.2975,  0.0106, -0.1670,  ...,  0.1100, -0.0406, -0.0155]],\n",
      "       grad_fn=<LinearFunctionBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0914, -1.4854,  0.1134,  ..., -0.2377,  1.0037, -0.7683],\n",
       "        [ 0.4085,  0.1006, -0.6509,  ...,  1.7163,  0.9196, -0.5435],\n",
       "        [-0.5303,  0.8741,  1.5429,  ..., -1.2192,  0.7187,  0.7971],\n",
       "        ...,\n",
       "        [-2.1106,  0.2159, -0.4652,  ...,  0.4250, -1.3479, -0.2294],\n",
       "        [ 0.9369, -0.7744,  2.0606,  ..., -0.8292, -0.5899, -0.1206],\n",
       "        [-1.0319,  0.6146, -0.3172,  ...,  0.2297, -1.4789,  1.0998]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss_gradient.shape)\n",
    "print(param[2])\n",
    "print(first_layer_x)\n",
    "loss_gradient.matmul(param[2]) + dumb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0478, -0.0918, -0.0705],\n",
      "        [ 0.0702,  0.0622, -0.0562],\n",
      "        [-0.0820, -0.0493,  0.0142],\n",
      "        [-0.0884,  0.0755, -0.0355],\n",
      "        [-0.0109,  0.0869, -0.0419],\n",
      "        [ 0.0202,  0.0658, -0.0808],\n",
      "        [ 0.0736, -0.0859, -0.0946],\n",
      "        [ 0.0393, -0.0757,  0.0435],\n",
      "        [-0.0081, -0.0914,  0.0919],\n",
      "        [ 0.0432, -0.0647, -0.0833],\n",
      "        [-0.0751, -0.0023,  0.0267],\n",
      "        [-0.0642,  0.0523,  0.0440],\n",
      "        [-0.0812,  0.0039, -0.0312],\n",
      "        [-0.0841, -0.0803, -0.0699],\n",
      "        [-0.0367,  0.0693, -0.0602],\n",
      "        [ 0.0396,  0.0787,  0.0327],\n",
      "        [-0.0245, -0.0190, -0.0222],\n",
      "        [-0.0507,  0.0758, -0.0181],\n",
      "        [-0.0978, -0.0492, -0.0030],\n",
      "        [ 0.0631, -0.0086, -0.0760],\n",
      "        [-0.0721, -0.0774, -0.0514],\n",
      "        [ 0.0171, -0.0902, -0.0275],\n",
      "        [-0.0265, -0.0178, -0.0691],\n",
      "        [ 0.0136,  0.0746,  0.0173],\n",
      "        [ 0.0158, -0.0189,  0.0811],\n",
      "        [ 0.0393, -0.0257, -0.0180],\n",
      "        [-0.0094,  0.0181, -0.0572],\n",
      "        [-0.0831, -0.0761, -0.0892],\n",
      "        [ 0.0936, -0.0112,  0.0898],\n",
      "        [-0.0724,  0.0423,  0.0751],\n",
      "        [ 0.0255, -0.0630,  0.0407],\n",
      "        [ 0.0617,  0.0247, -0.0901],\n",
      "        [-0.0685,  0.0848,  0.0212],\n",
      "        [-0.0569, -0.0982,  0.0729],\n",
      "        [ 0.0804, -0.0691,  0.0718],\n",
      "        [ 0.0875, -0.0643, -0.0758],\n",
      "        [-0.0954, -0.0962,  0.0298],\n",
      "        [-0.0098,  0.0417,  0.0322],\n",
      "        [ 0.0999, -0.0207,  0.0944],\n",
      "        [-0.0974,  0.0092,  0.0862],\n",
      "        [-0.0915,  0.0409, -0.0379],\n",
      "        [-0.0044,  0.0142, -0.0123],\n",
      "        [-0.0959,  0.0144,  0.0160],\n",
      "        [-0.0169, -0.0938,  0.0025],\n",
      "        [-0.0081, -0.0841, -0.0215],\n",
      "        [-0.0441, -0.0332, -0.0608],\n",
      "        [ 0.0434,  0.0060, -0.0994],\n",
      "        [ 0.0590,  0.0309, -0.0804],\n",
      "        [ 0.0648, -0.0198, -0.0839],\n",
      "        [-0.0940, -0.0480, -0.0475],\n",
      "        [ 0.0708,  0.0230,  0.0675],\n",
      "        [ 0.0578, -0.0699,  0.0427],\n",
      "        [-0.0415, -0.0413, -0.0310],\n",
      "        [-0.0814, -0.0929, -0.0103],\n",
      "        [ 0.0561, -0.0401, -0.0449],\n",
      "        [ 0.0870,  0.0274, -0.0613],\n",
      "        [-0.0607, -0.0679,  0.0551],\n",
      "        [-0.0621,  0.0660,  0.0525],\n",
      "        [ 0.0842, -0.0169,  0.0563],\n",
      "        [-0.0921, -0.0914, -0.0816],\n",
      "        [ 0.0302, -0.0698, -0.0938],\n",
      "        [ 0.0797, -0.0163,  0.0324],\n",
      "        [-0.0748,  0.0791, -0.0657],\n",
      "        [-0.0092, -0.0287, -0.0116]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-8.7387e-02, -6.5610e-02, -4.9968e-02,  1.5743e-02,  4.9888e-02,\n",
      "        -9.9040e-02, -7.4264e-02,  8.7115e-02, -1.2452e-02,  3.0232e-02,\n",
      "        -5.9713e-02, -2.7375e-02,  8.2687e-05, -2.6920e-03, -6.4055e-02,\n",
      "        -1.8675e-03, -8.2144e-02,  4.2855e-02, -8.4406e-02, -8.2786e-02,\n",
      "        -7.5640e-02, -1.9001e-02,  5.4754e-02,  5.5411e-02,  1.8831e-02,\n",
      "         1.5965e-02,  9.4349e-02, -4.0569e-02, -5.4621e-02, -6.3452e-02,\n",
      "         1.6583e-02,  6.8048e-02, -6.0000e-02, -5.5309e-02,  4.5165e-02,\n",
      "         9.5145e-04, -9.3800e-02,  8.0518e-02,  7.8180e-03, -4.5254e-02,\n",
      "         8.1170e-02,  8.8343e-03, -2.7363e-02, -8.6504e-02, -8.3036e-02,\n",
      "         8.5883e-02,  8.1198e-02,  8.4519e-03, -8.6013e-02,  9.9839e-02,\n",
      "         5.9118e-02, -7.4482e-02, -3.4768e-02,  9.1266e-02, -4.1165e-02,\n",
      "         4.5992e-02, -8.8307e-02, -9.6068e-02, -3.8217e-02,  7.1377e-02,\n",
      "         4.1982e-02,  1.4249e-02,  2.0819e-02,  3.4092e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0806, -0.0817, -0.0623,  ...,  0.0449,  0.0076, -0.0028],\n",
      "        [ 0.0179, -0.0492,  0.0152,  ..., -0.0833,  0.0754, -0.0307],\n",
      "        [ 0.0373,  0.0031, -0.0319,  ...,  0.0046, -0.0867,  0.0502],\n",
      "        ...,\n",
      "        [-0.0001,  0.0490,  0.0659,  ..., -0.0413, -0.0097, -0.0660],\n",
      "        [ 0.0660, -0.0291, -0.0094,  ..., -0.0817, -0.0620,  0.0590],\n",
      "        [ 0.0987,  0.0350,  0.0048,  ..., -0.0320, -0.0627, -0.0613]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0609, -0.0715, -0.0555, -0.0142,  0.0428, -0.0135, -0.0943, -0.0931,\n",
      "        -0.0974,  0.0571, -0.0761, -0.0340, -0.0283,  0.0952, -0.0084, -0.0722,\n",
      "        -0.0894, -0.0135, -0.0615, -0.0908, -0.0120, -0.0652, -0.0699, -0.0622,\n",
      "         0.0069, -0.0169,  0.0671,  0.0422,  0.0494, -0.0188, -0.0820,  0.0109,\n",
      "         0.0901, -0.0433, -0.0597, -0.0313,  0.0084, -0.0784, -0.0921, -0.0412,\n",
      "        -0.0985,  0.0152, -0.0693, -0.0527,  0.0391,  0.0914, -0.0072, -0.0837,\n",
      "         0.0974, -0.0159,  0.0441,  0.0010,  0.0007,  0.0029, -0.0266,  0.0773,\n",
      "         0.0132,  0.0195, -0.0762,  0.0605,  0.0599,  0.0169,  0.0659,  0.0611,\n",
      "        -0.0070, -0.0437,  0.0640, -0.0767,  0.0394, -0.0776,  0.0304,  0.0358,\n",
      "         0.0060, -0.0373,  0.0665, -0.0280,  0.0280, -0.0801,  0.0657, -0.0688,\n",
      "         0.0529,  0.0927, -0.0037, -0.0716,  0.0348,  0.0488,  0.0918, -0.0877,\n",
      "         0.0630,  0.0235,  0.0973,  0.0311,  0.0372,  0.0065, -0.0851, -0.0942,\n",
      "         0.0829,  0.0296, -0.0701,  0.0143,  0.0569,  0.0264,  0.0398, -0.0145,\n",
      "        -0.0250, -0.0363,  0.0933, -0.0986,  0.0421, -0.0352, -0.0953,  0.0476,\n",
      "        -0.0617,  0.0760, -0.0845, -0.0741, -0.0251, -0.0823,  0.0931, -0.0933,\n",
      "         0.0495,  0.0069, -0.0643, -0.0449,  0.0585,  0.0458,  0.0354,  0.0918],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in model.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.nn.Parameter(torch.randn([100,64]))\n",
    "x = torch.randn([3,64])\n",
    "b = torch.nn.Parameter(torch.randn([100]))\n",
    "output = (x.matmul(w.T) + b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(output, torch.ones([3,100]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss_gradient = ((output - torch.ones([3, 100]))/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.1762e-02, -6.3776e-03,  7.6651e-01,  6.4886e-01,  4.2851e-01,\n",
      "          4.1092e-01,  7.7465e-01,  5.7314e-02,  4.7015e-01,  2.6201e-01,\n",
      "          1.4099e-01,  8.8756e-02,  1.6239e-02,  3.1141e-02,  1.4890e-01,\n",
      "          1.1460e-03,  4.6536e-02,  1.1856e+00,  2.2837e-01,  1.1026e+00,\n",
      "          3.4822e-02, -6.0711e-03,  1.1283e-01,  1.9060e-01,  4.4046e-02,\n",
      "          4.0397e-02,  1.2414e-01,  1.3519e-01,  3.5951e-01,  1.7745e+00,\n",
      "          2.0226e-01,  2.9339e-01, -1.1761e-03,  7.1945e-01,  1.8567e-01,\n",
      "          2.1091e+00,  2.3302e-02,  1.2636e+00,  1.2043e+00,  1.1705e+00,\n",
      "          1.5859e+00,  1.8218e-01,  3.1120e-02,  1.1143e+00,  4.4103e-02,\n",
      "          6.7623e-01,  1.7845e+00,  1.0779e+00,  1.2061e+00,  1.2667e+00,\n",
      "          3.4298e-01,  3.3192e-01,  1.0344e+00,  1.1039e-02,  1.8017e-01,\n",
      "          4.8738e-01,  6.7580e-02, -5.2218e-03,  1.1003e+00,  3.3413e-01,\n",
      "          2.7089e-01,  1.4560e+00,  4.8384e-02,  1.8120e-01,  8.7348e-02,\n",
      "          1.3658e-01,  3.6332e-01,  1.1039e+00,  2.1671e-02, -1.4369e-03,\n",
      "          3.6303e-02, -5.3778e-03,  2.3875e-02,  1.6831e+00, -6.3603e-03,\n",
      "          3.1849e-01,  3.9940e-01,  1.3781e-01,  1.6178e+00,  2.0979e-01,\n",
      "          1.0897e-01,  7.8411e-01,  6.0341e-02, -1.9008e-03, -5.7586e-03,\n",
      "          9.5405e-04,  1.3162e+00,  7.5858e-01, -6.5249e-03,  1.8502e-01,\n",
      "          2.7409e-02,  6.1097e-01,  1.0765e-01,  9.0658e-01,  6.1967e-01,\n",
      "          2.5913e+00,  2.7741e-02, -1.6079e-03,  3.0033e-02,  3.7600e-01],\n",
      "        [ 3.4234e-02,  1.1311e+00,  1.0721e-01,  7.3568e-02,  6.0285e-01,\n",
      "          1.1131e+00, -2.8675e-03,  1.3656e-02,  1.9672e-02,  1.8552e+00,\n",
      "          8.2053e-01,  2.8145e-03,  9.7828e-02,  1.3567e+00,  3.6509e-01,\n",
      "          6.4805e-01,  3.6289e-02,  4.5289e-02,  5.2045e-03,  1.2537e+00,\n",
      "          5.0075e-01,  4.1800e-01,  7.7049e-01,  1.4605e-01,  4.0217e-03,\n",
      "          3.4966e-02,  8.6220e-02,  5.4579e-01,  3.7342e-02,  8.2000e-02,\n",
      "          1.5965e-01,  1.4937e+00,  7.3097e-02,  9.4429e-01,  7.1814e-03,\n",
      "          5.0748e-01,  6.2720e-01,  1.0057e-01,  8.2216e-01,  7.6070e-01,\n",
      "          1.7780e+00, -5.9011e-03, -3.5958e-03,  3.9066e-01,  1.6088e-01,\n",
      "          5.9627e-01,  3.0962e-01,  3.1770e-03,  3.3455e-01,  9.5775e-04,\n",
      "          1.4152e+00,  1.2746e-01,  5.2894e-01,  8.2274e-01,  1.7392e-01,\n",
      "          5.3752e-01,  3.0689e-01,  1.6028e-02,  6.7125e-01,  2.5430e-01,\n",
      "          3.1813e-01,  2.2992e-01,  8.3057e-03,  2.6996e+00, -5.9486e-03,\n",
      "          1.5670e-02,  1.7468e+00,  7.5920e-01,  3.7662e-01,  5.5974e-01,\n",
      "          4.5183e-01,  2.3280e-02,  4.7839e-01,  1.9753e-01,  4.1705e+00,\n",
      "          7.7756e-01, -6.5553e-03,  4.8522e-02,  2.1069e-01,  7.5383e-01,\n",
      "          6.3378e-01,  2.8671e-01,  8.6248e-03,  4.2544e-02,  3.5801e-01,\n",
      "          3.5024e-01,  1.4751e-01,  2.1688e+00,  7.0315e-02, -7.5864e-04,\n",
      "          3.9961e-01,  1.1573e+00,  1.3350e-01,  1.0125e+00,  3.1928e-03,\n",
      "          1.1107e+00,  9.3615e-02,  7.2702e-02,  1.3916e-02,  1.3003e-01],\n",
      "        [ 2.7828e-02,  3.4714e-02,  1.9582e+00,  3.1617e-02,  1.9869e-02,\n",
      "          2.5514e-03, -2.8807e-03, -5.5634e-03,  1.7442e-02, -5.6885e-03,\n",
      "         -5.5731e-03,  3.8475e-01,  1.2960e-01,  8.3770e-01,  7.0910e-03,\n",
      "          2.1917e-01,  3.6195e-02,  5.1497e-02,  2.9102e-03,  3.8453e-02,\n",
      "          1.7400e-02,  3.7580e-01,  8.3887e-01,  2.2414e-01,  6.6758e-02,\n",
      "          1.4957e-01,  9.5955e-02,  1.2731e+00,  2.1458e-01,  9.3858e-02,\n",
      "          9.9355e-01,  7.4808e-03,  1.8707e-01,  3.3642e-01,  2.5496e-02,\n",
      "          2.1570e-01,  3.3634e-02,  1.4728e-01,  3.0264e-01,  1.8079e-01,\n",
      "          1.2199e-01,  1.3251e-01, -2.4892e-03,  3.2288e-01,  4.8021e-01,\n",
      "          4.4206e-02,  4.1581e-03, -5.5992e-03,  1.5254e-01, -6.3603e-03,\n",
      "          1.6960e-01,  1.5549e-01, -4.8226e-03,  5.1025e-02,  4.2705e-02,\n",
      "         -6.6058e-03, -6.2695e-03,  1.1848e-01,  5.8262e-03,  1.2687e-01,\n",
      "          6.9222e-01,  7.2813e-02,  1.3391e-01,  7.3771e-02, -4.5529e-03,\n",
      "          4.4567e-04,  1.0170e-01,  1.2296e+00,  1.4227e-02,  2.9154e-02,\n",
      "          2.7013e-02,  5.8973e-01,  2.3811e+00,  1.7958e+00,  1.6282e-01,\n",
      "          4.8623e-01,  3.4646e-01,  3.1761e-01,  4.1855e-01,  8.0618e-01,\n",
      "          1.6274e-03,  2.4363e-01,  2.8439e-01,  6.5000e-02,  6.2749e-02,\n",
      "          7.1873e-01,  2.8469e-02,  3.3994e-01,  1.8051e+00,  8.7152e-03,\n",
      "          1.5531e-01, -4.9824e-03,  1.7944e-01,  5.7911e-02,  6.7732e-02,\n",
      "          1.0477e-01,  8.1460e-02,  1.7956e-02,  3.1697e-01,  4.7218e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(loss_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100])\n",
      "tensor([-4.3873e-01, -2.9728e+01, -4.9838e+01,  1.2509e+01, -4.6836e+00,\n",
      "        -2.2341e+01, -1.6781e+01,  3.9827e-01,  7.9640e+00,  5.8687e+01,\n",
      "         1.6957e+01, -5.2180e+00,  3.3703e-01,  2.0096e+01, -3.9937e+00,\n",
      "        -1.5398e+01, -1.0486e-01,  3.1658e+01,  2.7188e+00, -6.2306e+00,\n",
      "        -8.4976e+00, -9.8287e-01, -3.4577e+01, -8.3390e-01, -2.1032e-01,\n",
      "         1.8376e+00,  2.4963e+00,  4.6461e+01,  7.9930e+00,  5.8139e+01,\n",
      "        -2.3670e+01, -4.0903e+01,  2.5247e+00, -2.7125e+00,  2.0858e+00,\n",
      "        -6.3743e+01, -1.2298e+01, -3.2662e+01, -1.0005e+01,  1.2866e+01,\n",
      "         1.0230e+01,  3.1542e+00, -1.4912e-01, -1.8328e+01,  6.3512e+00,\n",
      "        -2.5274e+01,  6.2754e+01, -2.7495e+01, -2.6257e+01,  3.5017e+01,\n",
      "         4.8046e+01,  5.1212e+00,  1.6375e+01,  1.8017e+01, -3.2962e-01,\n",
      "        -1.8105e+01,  3.7614e+00, -1.0906e+00, -4.1908e+01,  2.7313e+00,\n",
      "         6.2380e+00,  3.9891e+01,  1.4830e+00,  1.1020e+02, -6.4700e-01,\n",
      "         1.3245e+00, -5.2067e+01, -2.1265e+01,  5.8511e+00,  1.0456e+01,\n",
      "         7.7998e+00,  1.1259e+01, -8.1861e+01, -3.2815e+00, -2.1043e+02,\n",
      "         2.0780e+01, -1.1279e+01,  3.4263e+00,  4.1416e+01,  6.8938e-01,\n",
      "         1.1520e+01,  1.6261e+01, -4.1147e+00, -6.6063e-01,  4.8950e+00,\n",
      "        -9.8713e+00, -3.8632e+01,  5.7199e+01,  5.9995e+01,  2.0122e+00,\n",
      "         4.8320e+00,  1.8827e+01, -2.2290e+00,  4.1760e+00,  1.2473e+01,\n",
      "        -7.4402e+01, -7.7766e-03,  5.6792e-01, -4.2272e+00, -1.1286e+00])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(loss_gradient.shape)\n",
    "    print(((loss_gradient*2*(x.matmul(w.T) + b))).sum(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.3873e-01, -2.9728e+01, -4.9838e+01,  1.2509e+01, -4.6836e+00,\n",
       "        -2.2341e+01, -1.6781e+01,  3.9827e-01,  7.9640e+00,  5.8687e+01,\n",
       "         1.6957e+01, -5.2180e+00,  3.3703e-01,  2.0096e+01, -3.9937e+00,\n",
       "        -1.5398e+01, -1.0486e-01,  3.1658e+01,  2.7188e+00, -6.2306e+00,\n",
       "        -8.4976e+00, -9.8287e-01, -3.4577e+01, -8.3390e-01, -2.1032e-01,\n",
       "         1.8376e+00,  2.4963e+00,  4.6461e+01,  7.9930e+00,  5.8139e+01,\n",
       "        -2.3670e+01, -4.0903e+01,  2.5247e+00, -2.7125e+00,  2.0858e+00,\n",
       "        -6.3743e+01, -1.2298e+01, -3.2662e+01, -1.0005e+01,  1.2866e+01,\n",
       "         1.0230e+01,  3.1542e+00, -1.4912e-01, -1.8328e+01,  6.3512e+00,\n",
       "        -2.5274e+01,  6.2754e+01, -2.7495e+01, -2.6257e+01,  3.5017e+01,\n",
       "         4.8046e+01,  5.1212e+00,  1.6375e+01,  1.8017e+01, -3.2962e-01,\n",
       "        -1.8105e+01,  3.7614e+00, -1.0906e+00, -4.1908e+01,  2.7313e+00,\n",
       "         6.2380e+00,  3.9891e+01,  1.4830e+00,  1.1020e+02, -6.4700e-01,\n",
       "         1.3245e+00, -5.2067e+01, -2.1265e+01,  5.8511e+00,  1.0456e+01,\n",
       "         7.7998e+00,  1.1259e+01, -8.1861e+01, -3.2815e+00, -2.1043e+02,\n",
       "         2.0780e+01, -1.1279e+01,  3.4263e+00,  4.1416e+01,  6.8938e-01,\n",
       "         1.1520e+01,  1.6261e+01, -4.1147e+00, -6.6063e-01,  4.8950e+00,\n",
       "        -9.8713e+00, -3.8632e+01,  5.7199e+01,  5.9995e+01,  2.0122e+00,\n",
       "         4.8320e+00,  1.8827e+01, -2.2290e+00,  4.1760e+00,  1.2473e+01,\n",
       "        -7.4402e+01, -7.7766e-03,  5.6792e-01, -4.2272e+00, -1.1286e+00])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PowBackward0 at 0x7fe4207078d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
